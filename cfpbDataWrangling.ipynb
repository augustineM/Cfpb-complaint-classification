{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section of code is general data wrangling and data management.  The objective of merging files, renaming, and organizing being further exploration and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
    "#read saved csv file\n",
    "conComplaintDf=pd.read_csv(\"Consumer_Complaints.csv\",converters={'ZIP code': lambda x: str(x)})\n",
    "\n",
    "#ccDf=conComplaintDf.groupby(['Company response to consumer']).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm going to recode the different resolutions as integers from 0-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#order the responses\n",
    "#0 closed with monetary relief\n",
    "#1 closed with non-monetary relief\n",
    "#2 closed with relief\n",
    "#3 closed with explanation\n",
    "#4 closed\n",
    "#5 closed without relief\n",
    "#6 untimely response\n",
    "#7 in progress\n",
    "conComplaintDf['respCode']=np.where(conComplaintDf['Company response to consumer']== 'Closed with monetary relief',0,\n",
    "    np.where(conComplaintDf['Company response to consumer']== 'Closed with non-monetary relief',1,\n",
    "        np.where(conComplaintDf['Company response to consumer']== 'Closed with relief',2,\n",
    "            np.where(conComplaintDf['Company response to consumer']== 'Closed with explanation',3,\n",
    "                np.where(conComplaintDf['Company response to consumer']== 'Closed',4,\n",
    "                    np.where(conComplaintDf['Company response to consumer']== 'Closed without relief',5,\n",
    "                        np.where(conComplaintDf['Company response to consumer']== 'Untimely response',6,\n",
    "                            np.where(conComplaintDf['Company response to consumer']== 'In progress',7,8))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean the date received field\n",
    "#convert 'date received' column from string to datetime\n",
    "#align all complaints to end of month \n",
    "#create a month-year column\n",
    "\n",
    "import calendar\n",
    "import datetime\n",
    "\n",
    "#conComplaintDfStg['Date received'].dtype\n",
    "conComplaintDf['dateRec']=pd.to_datetime(conComplaintDf['Date received'])#,format='%B/%d/%y')\n",
    "conComplaintDf['adjDate'] = conComplaintDf['dateRec'].map(\n",
    "    lambda x: datetime.datetime(\n",
    "        x.year,\n",
    "        x.month,\n",
    "        max(calendar.monthcalendar(x.year, x.month)[-1][:5])\n",
    "    )\n",
    ")\n",
    "conComplaintDf['monYear']=conComplaintDf['adjDate'].apply(lambda x: x.strftime('%B-%Y'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop rows without complete information(6&7) create data frames for data analysis\n",
    "\n",
    "#conComplaintDfStg=\n",
    "conComplaintDf.drop(conComplaintDf[conComplaintDf.respCode >=6].index, inplace=True)\n",
    "conComplaintDf['zip3']=conComplaintDf['ZIP code'].str[:3]\n",
    "\n",
    "\n",
    "#create dataframe with complaints resulting in monetary relief\n",
    "#response0Df=conComplaintDf.loc[(conComplaintDf.respCode== 0)]\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded and saved the CFPB complaint database but we could download from the website and load directly into a pandas dataframe.  Below I load census files directly into pandas dataframes from the websites they're hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download urban area to cbsa file\n",
    "#UA:urban area number\n",
    "#UANAME: urban area name\n",
    "#CBSA: corebased statistical area number(micro/metropolitan statistical area)\n",
    "#MNAME: cbsa name\n",
    "#MEMI: 1=metropolitan statistical area; 2=micropolitan statistical area\n",
    "url=\"https://www2.census.gov/geo/docs/maps-data/data/rel/ua_cbsa_rel_10.txt\"\n",
    "uaToCbsaDf=pd.read_csv(url,encoding='latin1')#converters={'ZCTA5': lambda x: str(x)})\n",
    "uaToCbsaDf=uaToCbsaDf[['UA','UANAME','CBSA','MNAME','MEMI','POPPT']]\n",
    "uaToCbsaDf.drop(uaToCbsaDf[(uaToCbsaDf.MEMI != 1) | (uaToCbsaDf.UA== 99999)|(uaToCbsaDf.CBSA==99999) ].index, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download zip to cbsa file\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "import io\n",
    "url=\"https://www2.census.gov/geo/docs/maps-data/data/rel/zcta_cbsa_rel_10.txt\"\n",
    "#zipToSaDf=pd.read_csv(url,'ZCTA5',index_col=0)\n",
    "#preserve leading zeros in zip5\n",
    "zipToSaDf=pd.read_csv(url,converters={'ZCTA5': lambda x: str(x)})\n",
    "zipToSaDf=zipToSaDf[['ZCTA5','CBSA','ZPOP','MEMI']]\n",
    "zipToSaDf.drop(zipToSaDf[(zipToSaDf.MEMI)!=1].index,inplace=True)\n",
    "zipToSaDf['zip3']=zipToSaDf.ZCTA5.str[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some records contain zip5 and some contain zip3.  This causes problems when merging\n",
    "#I will split the cfpb data into two files\n",
    "#1- with full 5-digit zip code\n",
    "#2- with 3-digit zip code\n",
    "conComplaintZ5=conComplaintDf[conComplaintDf['ZIP code'].str.contains(\"XX\")==False]\n",
    "\n",
    "conComplaintZ3=conComplaintDf[conComplaintDf['ZIP code'].str.contains(\"XX\")==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#later we will use zip codes to merge with census files(urban area and msa)\n",
    "#since zip3 is truncated zip5, they may link with multiple zip5s and UA/MSA\n",
    "#I want to remove the smaller(by population) instances of zip3\n",
    "#sort by zip3 and zip-level population\n",
    "#goal is to keep zip3 with largest population \n",
    "zipToSaDf.sort_values(['zip3','ZPOP'])\n",
    "zipToSaDfZ3=zipToSaDf.drop_duplicates(['zip3'],keep='last')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to merge the census files to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge the zip To CBSA file onto the split(by zip3 and zip5) consumer complaint data\n",
    "cfpbComplaintCbsaZ5= pd.merge(conComplaintZ5,zipToSaDf,left_on=['ZIP code','zip3'],right_on=['ZCTA5','zip3'],how='left')\n",
    "cfpbComplaintCbsaZ3= pd.merge(conComplaintZ3,zipToSaDfZ3,on=['zip3'],how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatenate the zip5 and zip3 files\n",
    "cfpbComplaintCbsa= pd.concat([cfpbComplaintCbsaZ5,cfpbComplaintCbsaZ3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UA to CBSA is a 'one to many' relationship. keep CBSA with largest population for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "uaToCbsaDf.sort_values(['CBSA','POPPT'],inplace=True)\n",
    "uaToCbsaDf=uaToCbsaDf.drop_duplicates(['CBSA'],keep='last')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through trial and error I found out I need to change the data type from float to int for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cfpbComplaintCbsa['CBSA']=cfpbComplaintCbsa['CBSA'].fillna(0.0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now merge with the uaCbsa data to add urban area information\n",
    "cfpbComplaintCbsaUa=pd.merge(cfpbComplaintCbsa,uaToCbsaDf, on='CBSA', how = 'inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading a zip file and loading into pandas is a bit different (and not as straight forward) than what I did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen   \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "URL = \\\n",
    "    'http://www2.census.gov/geo/docs/maps-data/data/gazetteer/Gaz_ua.zip'\n",
    "\n",
    "# open and save the zip file onto computer\n",
    "url = urlopen(URL)\n",
    "output = open('zipFile.zip', 'wb')    # note the flag:  \"wb\"        \n",
    "output.write(url.read())\n",
    "output.close()\n",
    "\n",
    "# read the zip file as a pandas dataframe\n",
    "uaGaz = pd.read_csv('zipFile.zip',sep='\\t',encoding='latin1')   # pandas version 0.18.1 takes zip files       \n",
    "\n",
    "# if keeping on disk the zip file is not wanted, then:\n",
    "#os.remove(zipName)   # remove the copy of the zipfile on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#keep ID, Lat, Long fields and merge onto cfpb data\n",
    "#uaGaz.count()\n",
    "#uaGaz=uaGaz[['GEOID','INTPTLAT','INTPTLONG']]\n",
    "gazCols=['GEOID','NAME',\n",
    " 'UATYPE',\n",
    " 'POP10',\n",
    " 'HU10',\n",
    " 'ALAND',\n",
    " 'AWATER',\n",
    " 'ALAND_SQMI',\n",
    " 'AWATER_SQMI',\n",
    " 'INTPTLAT',\n",
    " 'INTPTLONG']\n",
    "uaGaz.columns=gazCols\n",
    "cfpbComplaintCbsaUaLl= pd.merge(cfpbComplaintCbsaUa,uaGaz,left_on='UA',right_on='GEOID',how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfpbComplaintCbsaUaLl.iloc[:,-1]\n",
    "cfpbComplaintCbsaUaLl.rename(columns={'INTPTLONG\\n':'INTPTLONG'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfpbComplaintCbsaUaLl['target']=np.where(cfpbComplaintCbsaUaLl['respCode']== 0,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I take care of missing values and recode categorical values into binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Text processing replace na with an empty string\n",
    "cfpbComplaintCbsaUaLl['Consumer complaint narrative']= cfpbComplaintCbsaUaLl['Consumer complaint narrative'].fillna('')\n",
    "cfpbComplaintCbsaUaLl['State']= cfpbComplaintCbsaUaLl['State'].fillna('')\n",
    "#transform categorical variables into binary\n",
    "cfpbComplaintCbsaUaLl['UA']=cfpbComplaintCbsaUaLl['UA'].astype('category')\n",
    "#catCols= ['State','Product','Issue','UA']\n",
    "catCols= ['Product','Issue','UA']\n",
    "dfDummies= pd.get_dummies(cfpbComplaintCbsaUaLl[catCols])\n",
    "dataStg= pd.concat([cfpbComplaintCbsaUaLl,dfDummies],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to split the data into train and test sets (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train/test splitting\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#split dataStg into train/ test\n",
    "train, test= train_test_split(dataStg,test_size=.2)\n",
    "\n",
    "# recreate index in test and train sets so we can run through the text processing function\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
